{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Setup Notebook Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 - Run in Jupyter Bash Terminal\n",
    "```bash\n",
    "# create application-default credentials\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# construct spark_jars list\n",
    "spark_jars = [\"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar\"]\n",
    "if pyspark.version.__version__[0] == \"3\":\n",
    "    spark_jars.append(\"https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.12.jar\")\n",
    "else:\n",
    "    spark_jars.append(\"https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.11.jar\")\n",
    "\n",
    "# create SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \",\".join(spark_jars)) \\\n",
    "    .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"/home/jovyan/.config/gcloud/application_default_credentials.json\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Docs\n",
    "* https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "* https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"aaa\", 1, \"!!!\"),\n",
    "     (\"bbb\", 2, \"@@@\"),\n",
    "     (\"ccc\", 3, \"###\"),\n",
    "     (\"ddd\", 4, \"%%%\")],\n",
    "    schema=[\"col1\", \"col2\", \"col3\", ]\n",
    ")\n",
    "\n",
    "# write CSV\n",
    "out_uri = f\"gs://<<<MY_BUCKET>>>/example/spark_test.csv\"\n",
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(out_uri)\n",
    "\n",
    "# link to GUI\n",
    "print(\"----------------\")\n",
    "print(\"View in GUI:\")\n",
    "print(f\"https://console.cloud.google.com/storage/browser/${out_uri.lstrip('gs://')}/\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "in_uri = f\"gs://<<<MY_BUCKET>>>/example/spark_test.csv\"\n",
    "df2 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(in_uri)\n",
    "\n",
    "# view DataFrame\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Docs\n",
    "* https://github.com/GoogleCloudDataproc/spark-bigquery-connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Write to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame\n",
    "df3 = spark.createDataFrame(\n",
    "    [(\"aaa\", 1, \"!!!\"),\n",
    "     (\"bbb\", 2, \"@@@\"),\n",
    "     (\"ccc\", 3, \"###\"),\n",
    "     (\"ddd\", 4, \"%%%\")],\n",
    "    schema=[\"col1\", \"col2\", \"col3\", ]\n",
    ")\n",
    "\n",
    "# write to BigQuery\n",
    "out_project = \"<<<MY_PROJECT>>>\"\n",
    "out_table = \"<<<MY_DATABASE>>>.example__spark_notebook\"\n",
    "billing_project = \"<<<MY_PROJECT>>>\"\n",
    "df3.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"<<MY_BUCKET>>\") \\\n",
    "    .option(\"parentProject\", billing_project) \\\n",
    "    .option(\"project\", out_project) \\\n",
    "    .option(\"table\", out_table) \\\n",
    "    .save()\n",
    "\n",
    "# link to GUI\n",
    "print(\"----------------\")\n",
    "print(\"View in GUI:\")\n",
    "print(f\"https://console.cloud.google.com/bigquery?project=${out_project}\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Read from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from BigQuery\n",
    "in_project = \"<<<MY_PROJECT>>>\"\n",
    "in_table = \"<<<MY_DATABASE>>>.example__spark_notebook\"\n",
    "billing_project = \"<<<MY_PROJECT>>>\"\n",
    "df4 = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"readDataFormat\", \"ARROW\") \\\n",
    "    .option(\"parentProject\", billing_project) \\\n",
    "    .option(\"project\", in_project) \\\n",
    "    .option(\"table\", in_table) \\\n",
    "    .load()\n",
    "\n",
    "# view DataFrame\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Advanced Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Write File (Hadoop Java API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadoop_write_file(spark: SparkSession,\n",
    "                      fs_uri: str,\n",
    "                      overwrite: bool,\n",
    "                      file_data: str) -> str:\n",
    "    \"\"\"\n",
    "    Write a string as a file using the Hadoop Java API.\n",
    "\n",
    "    :param spark: a running SparkSession\n",
    "    :param fs_uri: the URI of the file\n",
    "    :param overwrite: if we should replace any existing file (error if False)\n",
    "    :param file_data: the string to write as the file data\n",
    "    :return the URI of the writen file\n",
    "    \"\"\"\n",
    "    # create py4j wrappers of java objects\n",
    "    hadoop = spark.sparkContext._jvm.org.apache.hadoop\n",
    "    java = spark.sparkContext._jvm.java\n",
    "\n",
    "    # create the FileSystem() object\n",
    "    conf = spark._jsc.hadoopConfiguration()\n",
    "    path = hadoop.fs.Path(java.net.URI(fs_uri))\n",
    "    fs = path.getFileSystem(conf)\n",
    "\n",
    "    # write the file\n",
    "    output_stream = fs.create(path, overwrite)\n",
    "    output_stream.writeBytes(file_data)\n",
    "    output_stream.close()\n",
    "\n",
    "    return fs_uri\n",
    "\n",
    "# write file\n",
    "out_uri = f\"gs://<<<MY_BUCKET>>>/example/spark_test.txt\"\n",
    "file_data = \"Hello World! \" * 100\n",
    "hadoop_write_file(spark=spark, fs_uri=out_uri, overwrite=True, file_data=file_data)\n",
    "\n",
    "# link to GUI\n",
    "print(\"----------------\")\n",
    "print(\"View in GUI:\")\n",
    "print(f\"https://console.cloud.google.com/storage/browser/${out_project.lstrip('gs://')}\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Read File (Hadoop Java API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadoop_read_file(spark: SparkSession,\n",
    "                     fs_uri: str,\n",
    "                     encoding: str = \"utf-8\") -> str:\n",
    "    \"\"\"\n",
    "    Read the content of a file as a string using the Hadoop Java API.\n",
    "\n",
    "    :param spark: a running SparkSession\n",
    "    :param fs_uri: the URI of the file\n",
    "    :param encoding: the file's encoding (defaults to utf-8)\n",
    "    :return: the content of the file (or None if the file is not present\n",
    "    \"\"\"\n",
    "    from py4j.protocol import Py4JJavaError\n",
    "\n",
    "    # create py4j wrappers of scala objects\n",
    "    commons = spark.sparkContext._jvm.org.apache.commons\n",
    "    hadoop = spark.sparkContext._jvm.org.apache.hadoop\n",
    "    java = spark.sparkContext._jvm.java\n",
    "\n",
    "    # create the FileSystem() object\n",
    "    conf = spark._jsc.hadoopConfiguration()\n",
    "    path = hadoop.fs.Path(java.net.URI(fs_uri))\n",
    "    fs = path.getFileSystem(conf)\n",
    "\n",
    "    # read file as string\n",
    "    try:\n",
    "        input_stream = fs.open(path)\n",
    "        file_data = commons.io.IOUtils.toString(input_stream, encoding)\n",
    "        input_stream.close()\n",
    "        return file_data\n",
    "    except Py4JJavaError as ex:\n",
    "        java_exception_class = ex.java_exception.getClass().getName()\n",
    "        if java_exception_class == \"java.io.FileNotFoundException\":\n",
    "            return None\n",
    "        else:\n",
    "            raise ex\n",
    "\n",
    "# read file\n",
    "in_uri = f\"gs://<<<MY_BUCKET>>>/example/spark_test.txt\"\n",
    "file_data = hadoop_read_file(spark=spark, fs_uri=in_uri)\n",
    "\n",
    "print(\"-------- File Content --------\")\n",
    "print(file_data)\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
